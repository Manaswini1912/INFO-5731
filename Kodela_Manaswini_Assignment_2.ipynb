{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manaswini1912/INFO-5731/blob/main/Kodela_Manaswini_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7b089b-2406-4c22-866b-105cfe3db9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting IMDbPY\n",
            "  Downloading IMDbPY-2022.7.9-py3-none-any.whl (1.2 kB)\n",
            "Collecting cinemagoer (from IMDbPY)\n",
            "  Downloading cinemagoer-2023.5.1-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy in /usr/local/lib/python3.10/dist-packages (from cinemagoer->IMDbPY) (2.0.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from cinemagoer->IMDbPY) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->cinemagoer->IMDbPY) (4.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy->cinemagoer->IMDbPY) (3.0.3)\n",
            "Installing collected packages: cinemagoer, IMDbPY\n",
            "Successfully installed IMDbPY-2022.7.9 cinemagoer-2023.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install IMDbPY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "def retrieve_reviews(movie_id):\n",
        "    base_url = f'https://www.imdb.com/title/tt15398776/reviews'\n",
        "    reviews_list = []\n",
        "    try:\n",
        "        while True:\n",
        "            response = requests.get(base_url)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            containers = soup.find_all('div', class_='review-container')\n",
        "\n",
        "            for container in containers:\n",
        "                title_element = container.find('a', class_='title')\n",
        "                title = title_element.get_text(strip=True) if title_element else 'No Title'\n",
        "\n",
        "                review_text_element = container.find('div', class_='text show-more__control')\n",
        "                review_text = review_text_element.get_text(strip=True) if review_text_element else 'No Review Text'\n",
        "\n",
        "                reviews_list.append((title, review_text))\n",
        "\n",
        "            has_more_reviews = len(reviews_list) < 1000 and containers\n",
        "            if not has_more_reviews:\n",
        "                break\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error occurred: {e}')\n",
        "\n",
        "    return reviews_list[:1000]\n",
        "\n",
        "movie_ids = ['tt15398776']\n",
        "all_reviews = []\n",
        "\n",
        "for movie_id in movie_ids:\n",
        "    all_reviews.extend(retrieve_reviews(movie_id))\n",
        "    if len(all_reviews) >= 1000:\n",
        "        break\n",
        "\n",
        "with open('movie_review_feedback.csv', 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Title', 'Review'])\n",
        "    for title, review in all_reviews:\n",
        "        writer.writerow([title, review])\n",
        "\n",
        "print(f'Successfully saved {len(all_reviews)} reviews with titles.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7f_VMpk0-X",
        "outputId": "30826d45-0836-420b-f11d-abb4e22e536d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 1000 reviews with titles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
        "    cleaned_text = ''.join(char for char in cleaned_text if not char.isdigit())\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_text = ' '.join(word for word in words if word.lower() not in stop_words)\n",
        "    return filtered_text\n",
        "\n",
        "def stem_text(text):\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_text = ' '.join(ps.stem(word) for word in text.split())\n",
        "    return stemmed_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "    return lemmatized_text\n",
        "\n",
        "def process_reviews(reviews_data):\n",
        "    processed_reviews = []\n",
        "    for title, review in reviews_data:\n",
        "        processed_title = clean_text(title)\n",
        "        processed_review = clean_text(review)\n",
        "        processed_reviews.append((processed_title, processed_review))\n",
        "    return processed_reviews\n",
        "\n",
        "# Input and output file paths\n",
        "input_file_path = 'movie_review_feedback.csv'\n",
        "output_file_path = 'cleaned_reviews.csv'\n",
        "\n",
        "# Read existing reviews from the input CSV file\n",
        "reviews_data = []\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        title, review = row\n",
        "        reviews_data.append((title, review))\n",
        "\n",
        "# Clean and process the reviews\n",
        "processed_reviews = process_reviews(reviews_data)\n",
        "\n",
        "# Save cleaned reviews to a new CSV file\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Cleaned Title', 'Cleaned Review'])  # Header\n",
        "    for processed_title, processed_review in processed_reviews:\n",
        "        writer.writerow([processed_title, processed_review])\n",
        "\n",
        "# Print the number of cleaned reviews saved\n",
        "print(f'Successfully saved {len(processed_reviews)} cleaned reviews in {output_file_path}.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV38zq3NjXW0",
        "outputId": "e051d160-ae70-493c-cec2-2e01b8a4cec9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 1000 cleaned reviews in cleaned_reviews.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebc4b16-531a-416e-f67c-1a587f078aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags Count: {'Noun': 60, 'Verb': 50, 'Adjective': 38, 'Adverb': 33}\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "one <--nsubj-- included\n",
            "of <--prep-- one\n",
            "the <--det-- films\n",
            "most <--advmod-- anticipated\n",
            "anticipated <--amod-- films\n",
            "films <--pobj-- of\n",
            "of <--prep-- films\n",
            "the <--det-- year\n",
            "year <--pobj-- of\n",
            "for <--prep-- films\n",
            "many <--amod-- people\n",
            "people <--pobj-- for\n",
            "myself <--appos-- people\n",
            "included <--ROOT-- included\n",
            "oppenheimer <--nsubj-- delivers\n",
            "largely <--advmod-- delivers\n",
            "delivers <--ccomp-- included\n",
            "much <--dobj-- delivers\n",
            "of <--prep-- much\n",
            "its <--poss-- great\n",
            "great <--pobj-- of\n",
            "i <--nsubj-- feel\n",
            "feel <--ccomp-- delivers\n",
            "like <--mark-- loved\n",
            "i <--nsubj-- loved\n",
            "loved <--advcl-- feel\n",
            "two <--dobj-- loved\n",
            "of <--prep-- two\n",
            "its <--poss-- hours\n",
            "three <--nummod-- hours\n",
            "hours <--pobj-- of\n",
            "and <--cc-- loved\n",
            "liked <--conj-- loved\n",
            "the <--det-- hour\n",
            "other <--amod-- hour\n",
            "hour <--dobj-- liked\n",
            "but <--cc-- delivers\n",
            "its <--poss-- fact\n",
            "that <--det-- fact\n",
            "fact <--conj-- delivers\n",
            "that <--nsubj-- stops\n",
            "stops <--acl-- fact\n",
            "me <--dobj-- stops\n",
            "from <--prep-- stops\n",
            "adoring <--pcomp-- from\n",
            "the <--det-- thing\n",
            "entire <--amod-- thing\n",
            "thing <--dobj-- adoring\n",
            "i <--nsubj-- know\n",
            "know <--relcl-- thing\n",
            "with <--prep-- adoring\n",
            "christopher <--compound-- nolans\n",
            "nolans <--compound-- dunkirk\n",
            "dunkirk <--pobj-- with\n",
            "that <--nsubj-- clicked\n",
            "clicked <--relcl-- me\n",
            "on <--prep-- clicked\n",
            "a <--det-- watch\n",
            "second <--amod-- watch\n",
            "watch <--pobj-- on\n",
            "so <--advmod-- need\n",
            "maybe <--advmod-- need\n",
            "oppenheimer <--nsubj-- need\n",
            "will <--aux-- need\n",
            "need <--ROOT-- need\n",
            "one <--dobj-- need\n",
            "too <--advmod-- need\n",
            "that <--mark-- said\n",
            "being <--auxpass-- said\n",
            "said <--ccomp-- need\n",
            "i <--nsubj-- feel\n",
            "do <--aux-- feel\n",
            "nt <--neg-- feel\n",
            "feel <--ccomp-- said\n",
            "the <--det-- need\n",
            "need <--dobj-- feel\n",
            "to <--aux-- rush\n",
            "rush <--acl-- need\n",
            "out <--prt-- rush\n",
            "and <--cc-- rush\n",
            "see <--conj-- rush\n",
            "it <--dobj-- see\n",
            "again <--advmod-- see\n",
            "too <--advmod-- soon\n",
            "soon <--advmod-- see\n",
            "because <--mark-- was\n",
            "it <--nsubj-- was\n",
            "was <--advcl-- see\n",
            "a <--det-- filmbut\n",
            "long <--amod-- filmbut\n",
            "and <--cc-- long\n",
            "exhausting <--conj-- long\n",
            "filmbut <--attr-- was\n",
            "in <--prep-- filmbut\n",
            "many <--amod-- ways\n",
            "ways <--pobj-- in\n",
            "i <--nsubj-- deny\n",
            "ca <--aux-- deny\n",
            "nt <--neg-- deny\n",
            "deny <--relcl-- filmbut\n",
            "it <--nsubj-- was\n",
            "was <--ccomp-- feel\n",
            "an <--det-- one\n",
            "exceptionally <--advmod-- well\n",
            "well <--advmod-- made\n",
            "made <--amod-- one\n",
            "one <--attr-- was\n",
            "it <--nsubj-- looks\n",
            "looks <--relcl-- one\n",
            "and <--cc-- looks\n",
            "sounds <--conj-- looks\n",
            "as <--advmod-- amazing\n",
            "amazing <--acomp-- sounds\n",
            "as <--mark-- expect\n",
            "you <--compound-- d\n",
            "d <--nsubj-- expect\n",
            "expect <--advcl-- amazing\n",
            "feeling <--dobj-- expect\n",
            "as <--mark-- captures\n",
            "though <--mark-- captures\n",
            "it <--nsubj-- captures\n",
            "accurately <--advmod-- captures\n",
            "captures <--advcl-- expect\n",
            "the <--det-- period\n",
            "time <--compound-- period\n",
            "period <--dobj-- captures\n",
            "its <--poss-- set\n",
            "set <--dobj-- captures\n",
            "in <--prep-- captures\n",
            "and <--cc-- captures\n",
            "containing <--conj-- captures\n",
            "amazing <--amod-- design\n",
            "sound <--amod-- design\n",
            "design <--dobj-- containing\n",
            "and <--cc-- design\n",
            "one <--conj-- design\n",
            "of <--prep-- one\n",
            "the <--det-- years\n",
            "years <--pobj-- of\n",
            "best <--amod-- scores\n",
            "scores <--dobj-- need\n",
            "so <--advmod-- far\n",
            "far <--advmod-- is\n",
            "every <--det-- performance\n",
            "performance <--nsubj-- is\n",
            "is <--conj-- need\n",
            "good <--acomp-- is\n",
            "to <--prep-- good\n",
            "great <--pobj-- to\n",
            "but <--cc-- need\n",
            "the <--det-- film\n",
            "film <--nsubj-- belongs\n",
            "belongs <--conj-- need\n",
            "to <--aux-- cillian\n",
            "cillian <--advcl-- belongs\n",
            "murphy <--dobj-- cillian\n",
            "and <--cc-- belongs\n",
            "i <--nsubj-- feel\n",
            "feel <--conj-- belongs\n",
            "like <--mark-- s\n",
            "he <--nsubj-- s\n",
            "s <--advcl-- feel\n",
            "the <--det-- actor\n",
            "lead <--amod-- actor\n",
            "actor <--attr-- s\n",
            "to <--aux-- beat\n",
            "beat <--xcomp-- s\n",
            "at <--prep-- beat\n",
            "this <--det-- stage\n",
            "stage <--pobj-- at\n",
            "if <--mark-- talking\n",
            "were <--aux-- talking\n",
            "talking <--advcl-- s\n",
            "early <--amod-- awards\n",
            "awards <--dobj-- talking\n",
            "considerationthe <--det-- films\n",
            "films <--advcl-- talking\n",
            "at <--prep-- films\n",
            "its <--poss-- best\n",
            "best <--pobj-- at\n",
            "when <--advmod-- focuses\n",
            "it <--nsubj-- focuses\n",
            "focuses <--advcl-- talking\n",
            "on <--prep-- focuses\n",
            "being <--pcomp-- on\n",
            "a <--det-- thriller\n",
            "psychological <--amod-- thriller\n",
            "thriller <--attr-- being\n",
            "featuring <--acl-- thriller\n",
            "a <--det-- figure\n",
            "famous <--amod-- figure\n",
            "historical <--amod-- figure\n",
            "figure <--dobj-- featuring\n",
            "and <--cc-- on\n",
            "at <--prep-- turns\n",
            "one <--nummod-- point\n",
            "point <--pobj-- at\n",
            "it <--nsubj-- turns\n",
            "even <--advmod-- turns\n",
            "turns <--conj-- belongs\n",
            "into <--prep-- turns\n",
            "a <--det-- film\n",
            "psychological <--amod-- horror\n",
            "horror <--compound-- film\n",
            "film <--pobj-- into\n",
            "there <--expl-- s\n",
            "s <--relcl-- film\n",
            "one <--nummod-- sequence\n",
            "sequence <--attr-- s\n",
            "in <--prep-- sequence\n",
            "here <--pcomp-- in\n",
            "involving <--acl-- sequence\n",
            "a <--det-- speech\n",
            "speech <--dobj-- involving\n",
            "that <--nsubj-- terrifying\n",
            "s <--aux-- terrifying\n",
            "particularly <--advmod-- terrifying\n",
            "terrifying <--relcl-- speech\n",
            "it <--nsubj-- manages\n",
            "also <--advmod-- manages\n",
            "manages <--ccomp-- turns\n",
            "to <--aux-- have\n",
            "have <--xcomp-- manages\n",
            "some <--det-- moments\n",
            "very <--advmod-- suspenseful\n",
            "suspenseful <--amod-- moments\n",
            "moments <--dobj-- have\n",
            "even <--advmod-- known\n",
            "though <--mark-- known\n",
            "its <--poss-- story\n",
            "story <--nsubjpass-- known\n",
            "is <--auxpass-- known\n",
            "commonly <--advmod-- known\n",
            "known <--amod-- history\n",
            "history <--nsubj-- feel\n",
            "at <--prep-- history\n",
            "this <--det-- pointi\n",
            "pointi <--pobj-- at\n",
            "did <--aux-- feel\n",
            "really <--advmod-- feel\n",
            "feel <--ccomp-- turns\n",
            "the <--det-- length\n",
            "length <--dobj-- feel\n",
            "in <--prep-- length\n",
            "the <--det-- hour\n",
            "final <--amod-- hour\n",
            "hour <--pobj-- in\n",
            "though <--advmod-- feel\n",
            "and <--cc-- need\n",
            "maybe <--advmod-- wish\n",
            "i <--nsubj-- wish\n",
            "wish <--conj-- need\n",
            "that <--mark-- been\n",
            "final <--amod-- act\n",
            "act <--nsubj-- been\n",
            "had <--aux-- been\n",
            "been <--ccomp-- wish\n",
            "more <--attr-- been\n",
            "of <--prep-- more\n",
            "an <--det-- epilogue\n",
            "extended <--amod-- epilogue\n",
            "epilogue <--pobj-- of\n",
            "rather <--advmod-- than\n",
            "than <--cc-- more\n",
            "a <--det-- third\n",
            "whole <--amod-- third\n",
            "third <--conj-- more\n",
            "of <--prep-- third\n",
            "the <--det-- movie\n",
            "movie <--pobj-- of\n",
            "i <--nsubj-- feel\n",
            "currently <--advmod-- feel\n",
            "feel <--relcl-- movie\n",
            "as <--mark-- loved\n",
            "though <--mark-- loved\n",
            "i <--nsubj-- loved\n",
            "would <--aux-- loved\n",
            "ve <--aux-- loved\n",
            "loved <--advcl-- feel\n",
            "oppenheimer <--dobj-- loved\n",
            "more <--advmod-- loved\n",
            "had <--aux-- been\n",
            "it <--nsubj-- been\n",
            "been <--ccomp-- wish\n",
            "  <--dep-- been\n",
            "hours <--attr-- been\n",
            "instead <--advmod-- of\n",
            "of <--cc-- been\n",
            "  <--dep-- of\n",
            "but <--cc-- need\n",
            "nothing <--nsubj-- was\n",
            "about <--prep-- nothing\n",
            "it <--pobj-- about\n",
            "was <--conj-- need\n",
            "bad <--acomp-- was\n",
            "by <--prep-- was\n",
            "any <--det-- means\n",
            "means <--pobj-- by\n",
            "just <--advmod-- patience\n",
            "a <--det-- patience\n",
            "little <--amod-- patience\n",
            "patience <--compound-- testing\n",
            "testing <--advcl-- was\n",
            "this <--nsubj-- is\n",
            "is <--conj-- need\n",
            "very <--advmod-- subjective\n",
            "subjective <--acomp-- is\n",
            "  <--dep-- subjective\n",
            "i <--nsubj-- remember\n",
            "remember <--conj-- need\n",
            "feeling <--xcomp-- remember\n",
            "like <--prep-- feeling\n",
            "the <--det-- babylon\n",
            "similarly <--advmod-- long\n",
            "long <--amod-- babylon\n",
            "babylon <--nsubj-- justified\n",
            "totally <--advmod-- justified\n",
            "justified <--ccomp-- remember\n",
            "its <--poss-- runtime\n",
            "runtime <--dobj-- justified\n",
            "though <--mark-- feel\n",
            "others <--nsubj-- feel\n",
            "did <--aux-- feel\n",
            "nt <--neg-- feel\n",
            "feel <--advcl-- justified\n",
            "that <--mark-- left\n",
            "wayim <--nsubj-- left\n",
            "left <--ccomp-- feel\n",
            "feeling <--dobj-- left\n",
            "like <--mark-- watched\n",
            "i <--nsubj-- watched\n",
            "watched <--advcl-- left\n",
            "a <--det-- film\n",
            "film <--dobj-- watched\n",
            "that <--nsubj-- was\n",
            "was <--relcl-- film\n",
            "nt <--neg-- was\n",
            "a <--det-- dunk\n",
            "slam <--compound-- dunk\n",
            "dunk <--attr-- was\n",
            "but <--cc-- was\n",
            "was <--conj-- was\n",
            "incredible <--acomp-- was\n",
            "for <--prep-- was\n",
            "more <--pobj-- for\n",
            "of <--prep-- more\n",
            "its <--poss-- runtime\n",
            "runtime <--pobj-- of\n",
            "than <--mark-- was\n",
            "it <--nsubj-- was\n",
            "was <--advcl-- was\n",
            "nt <--punct-- need\n",
            "and <--cc-- s\n",
            "that <--nsubj-- s\n",
            "s <--ROOT-- s\n",
            "still <--advmod-- s\n",
            "worth <--acomp-- s\n",
            "celebrating <--xcomp-- worth\n",
            "and <--cc-- s\n",
            "makes <--conj-- s\n",
            "oppenheimer <--nsubj-- worth\n",
            "worth <--ccomp-- makes\n",
            "seeing <--acl-- worth\n",
            "in <--prep-- seeing\n",
            "cinemas <--pobj-- in\n",
            "for <--prep-- seeing\n",
            "sure <--amod-- for\n",
            "\n",
            "Named Entity Recognition (NER) Count:\n",
            "PERSON: 2\n",
            "ORG: 0\n",
            "LOC: 0\n",
            "PRODUCT: 0\n",
            "DATE: 1\n",
            "CARDINAL: 5\n",
            "TIME: 3\n",
            "ORDINAL: 2\n",
            "NORP: 1\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Read the cleaned reviews from the CSV file\n",
        "cleaned_reviews = []\n",
        "\n",
        "with open('cleaned_reviews.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        cleaned_title, cleaned_review = row\n",
        "        cleaned_reviews.append((cleaned_title, cleaned_review))\n",
        "\n",
        "# Choose a sample review for analysis\n",
        "sample_review = cleaned_reviews[0][1]  # Choose the review text from the first row\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "doc = nlp(sample_review)\n",
        "pos_tags_count = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        pos_tags_count['Noun'] += 1\n",
        "    elif token.pos_ == 'VERB':\n",
        "        pos_tags_count['Verb'] += 1\n",
        "    elif token.pos_ == 'ADJ':\n",
        "        pos_tags_count['Adjective'] += 1\n",
        "    elif token.pos_ == 'ADV':\n",
        "        pos_tags_count['Adverb'] += 1\n",
        "\n",
        "print('POS Tags Count:', pos_tags_count)\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "# Print constituency parsing tree\n",
        "print('\\nConstituency Parsing Tree:')\n",
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "entities_count = {'PERSON': 0, 'ORG': 0, 'LOC': 0, 'PRODUCT': 0, 'DATE': 0}\n",
        "\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ in entities_count:\n",
        "        entities_count[ent.label_] += 1\n",
        "    else:\n",
        "        entities_count[ent.label_] = 1  # If the key doesn't exist, create it and set count to 1\n",
        "\n",
        "\n",
        "print('\\nNamed Entity Recognition (NER) Count:')\n",
        "for entity, count in entities_count.items():\n",
        "    print(f\"{entity}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I tried to complete at my best with all my efforts, but I found questions quite challenging with the included concepts. But I found Retreiving the reviews of a movie is quite interesting and fun to do."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}