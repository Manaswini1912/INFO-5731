{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manaswini1912/INFO-5731/blob/main/Kodela_Manaswini_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zCjFaLqW9tzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the integration of artificial intelligence (AI) algorithms in healthcare diagnostic processes affect patient outcomes and medical professionals' workload?\n",
        "\n",
        "# 1. Obtain permission and approval from ethics committees or institutional review boards.\n",
        "# 2. Identify data sources, such as hospitals, clinics, insurance companies, and research databases.\n",
        "# 3. Collect patient demographic information, medical history, and treatment outcomes from electronic health records.\n",
        "# 4. Gather health monitoring data from wearable devices and sensors.\n",
        "# 5. Obtain billing records, insurance claims data, and healthcare expenditure information.\n",
        "# 6. Capture outputs generated by AI algorithms, including diagnostic predictions and treatment recommendations.\n",
        "# 7. Track the usage of AI algorithms in clinical decision-making.\n",
        "# 8. Measure clinical outcomes and patient satisfaction through surveys or interviews.\n",
        "# 9. Ensure adherence to ethical guidelines, obtain informed consent, and protect patient confidentiality.\n",
        "# 10. Integrate data from different sources into a centralized database.\n",
        "# 11. Cleanse and preprocess the data to remove errors and inconsistencies.\n",
        "# 12. Store collected data securely using databases, data warehouses, or cloud storage.\n",
        "# 13. Implement access controls and encryption to protect data integrity and confidentiality.\n",
        "# 14. Document data sources, collection methods, and preprocessing steps.\n",
        "# 15. Backup data regularly and establish disaster recovery plans.\n",
        "# 16. Define access policies and permissions for data sharing with authorized individuals or research collaborators.\n",
        "# 17. Ensure compliance with data sharing agreements and intellectual property rights."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to collect patient data\n",
        "def collect_patient_data(num_samples):\n",
        "    patient_data = []\n",
        "    for _ in range(num_samples):\n",
        "        # Simulate patient data collection process\n",
        "        # Replace this with actual data collection code\n",
        "        patient_info = {\n",
        "            \"patient_id\": \"P\" + str(_),\n",
        "            \"age\": 30 + _ % 50,\n",
        "            \"gender\": \"Male\" if _ % 2 == 0 else \"Female\",\n",
        "            \"diagnosis\": \"Hypertension\" if _ % 3 == 0 else \"Diabetes\",\n",
        "            \"treatment\": \"Medication\" if _ % 2 == 0 else \"Lifestyle changes\",\n",
        "            \"outcome\": \"Improved\" if _ % 4 == 0 else \"Stable\"\n",
        "        }\n",
        "        patient_data.append(patient_info)\n",
        "    return patient_data\n",
        "\n",
        "# Function to collect AI algorithm performance data\n",
        "def collect_algorithm_data(num_samples):\n",
        "    algorithm_data = []\n",
        "    for _ in range(num_samples):\n",
        "        # Simulate algorithm performance data collection process\n",
        "        # Replace this with actual data collection code\n",
        "        algorithm_info = {\n",
        "            \"patient_id\": \"P\" + str(_),\n",
        "            \"diagnostic_prediction\": \"High\" if _ % 3 == 0 else \"Low\",\n",
        "            \"treatment_recommendation\": \"Medication\" if _ % 2 == 0 else \"Lifestyle changes\",\n",
        "            \"usage_frequency\": _ % 5\n",
        "        }\n",
        "        algorithm_data.append(algorithm_info)\n",
        "    return algorithm_data\n",
        "\n",
        "# Collect patient data\n",
        "patient_data = collect_patient_data(1000)\n",
        "\n",
        "# Collect AI algorithm performance data\n",
        "algorithm_data = collect_algorithm_data(1000)\n",
        "\n",
        "# Convert data to pandas DataFrame\n",
        "patient_df = pd.DataFrame(patient_data)\n",
        "algorithm_df = pd.DataFrame(algorithm_data)\n",
        "\n",
        "# Merge patient data and algorithm performance data based on patient ID\n",
        "merged_df = pd.merge(patient_df, algorithm_df, on=\"patient_id\")\n",
        "\n",
        "# Save merged dataset to CSV file\n",
        "merged_df.to_csv(\"healthcare_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetch_google_scholar_articles(keyword, max_articles=1000):\n",
        "    base_url = \"https://scholar.google.com\"\n",
        "    articles = []\n",
        "\n",
        "    # Iterate over search result pages until desired number of articles is collected\n",
        "    while len(articles) < max_articles:\n",
        "        # Construct the search query URL\n",
        "        url = f\"{base_url}/scholar?q={keyword}&hl=en&as_sdt=0%2C5&as_ylo=2014&as_yhi=2024&start={len(articles)}\"\n",
        "\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract article elements\n",
        "        article_elements = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "        # Iterate over each article element\n",
        "        for article_element in article_elements:\n",
        "            try:\n",
        "                # Extract article information\n",
        "                title = article_element.find(\"h3\", class_=\"gs_rt\").text.strip()\n",
        "                venue = article_element.find(\"div\", class_=\"gs_a\").text.split(\" - \")[0].strip()\n",
        "                year = article_element.find(\"div\", class_=\"gs_a\").text.split(\" - \")[-1].split(\",\")[-1].strip()\n",
        "                authors = \", \".join(article_element.find(\"div\", class_=\"gs_a\").text.split(\" - \")[-1].split(\",\")[:-1]).strip()\n",
        "                abstract = article_element.find(\"div\", class_=\"gs_rs\").text.strip()\n",
        "\n",
        "                # Append article information to the list\n",
        "                articles.append({\n",
        "                    \"title\": title,\n",
        "                    \"venue\": venue,\n",
        "                    \"year\": year,\n",
        "                    \"authors\": authors,\n",
        "                    \"abstract\": abstract\n",
        "                })\n",
        "\n",
        "                # If desired number of articles is reached, break the loop\n",
        "                if len(articles) == max_articles:\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing article: {e}\")\n",
        "\n",
        "        # Add a delay to avoid hitting the server too frequently\n",
        "        time.sleep(2)\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Fetch articles from Google Scholar\n",
        "articles = fetch_google_scholar_articles(\"XYZ\", max_articles=1000)\n",
        "\n",
        "# Print the first article to check\n",
        "print(articles[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "class SocialMediaDataCollector:\n",
        "    def __init__(self, api_keys):\n",
        "        self.api_keys = api_keys\n",
        "\n",
        "    def fetch_data(self, platform, query):\n",
        "        if platform.lower() == 'reddit':\n",
        "            return self.fetch_reddit_data(query)\n",
        "        elif platform.lower() == 'instagram':\n",
        "            return self.fetch_instagram_data(query)\n",
        "        elif platform.lower() == 'twitter':\n",
        "            return self.fetch_twitter_data(query)\n",
        "        elif platform.lower() == 'facebook':\n",
        "            return self.fetch_facebook_data(query)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported platform\")\n",
        "\n",
        "    def fetch_reddit_data(self, query):\n",
        "        # Placeholder implementation - Replace with actual Reddit API logic\n",
        "        return [\n",
        "            {\"USER ID\": \"reddit_user1\", \"USERNAME\": \"RedditUser1\", \"CREATED AT\": \"2024-02-15\", \"PROFILE NAME\": \"reddit_profile1\", \"TEXT\": \"Sample text 1\"},\n",
        "            {\"USER ID\": \"reddit_user2\", \"USERNAME\": \"RedditUser2\", \"CREATED AT\": \"2024-02-14\", \"PROFILE NAME\": \"reddit_profile2\", \"TEXT\": \"Sample text 2\"}\n",
        "        ]\n",
        "\n",
        "    def fetch_instagram_data(self, query):\n",
        "        # Placeholder implementation - Replace with actual Instagram API logic\n",
        "        return [\n",
        "            {\"USER ID\": \"instagram_user1\", \"USERNAME\": \"InstagramUser1\", \"CREATED AT\": \"2024-02-15\", \"PROFILE NAME\": \"instagram_profile1\", \"TEXT\": \"Sample text 1\"},\n",
        "            {\"USER ID\": \"instagram_user2\", \"USERNAME\": \"InstagramUser2\", \"CREATED AT\": \"2024-02-14\", \"PROFILE NAME\": \"instagram_profile2\", \"TEXT\": \"Sample text 2\"}\n",
        "        ]\n",
        "\n",
        "    def fetch_twitter_data(self, query):\n",
        "        # Placeholder implementation - Replace with actual Twitter API logic\n",
        "        return [\n",
        "            {\"USER ID\": \"twitter_user1\", \"USERNAME\": \"TwitterUser1\", \"CREATED AT\": \"2024-02-15\", \"PROFILE NAME\": \"twitter_profile1\", \"TEXT\": \"Sample text 1\"},\n",
        "            {\"USER ID\": \"twitter_user2\", \"USERNAME\": \"TwitterUser2\", \"CREATED AT\": \"2024-02-14\", \"PROFILE NAME\": \"twitter_profile2\", \"TEXT\": \"Sample text 2\"}\n",
        "        ]\n",
        "\n",
        "    def fetch_facebook_data(self, query):\n",
        "        # Placeholder implementation - Replace with actual Facebook API logic\n",
        "        return [\n",
        "            {\"USER ID\": \"facebook_user1\", \"USERNAME\": \"FacebookUser1\", \"CREATED AT\": \"2024-02-15\", \"PROFILE NAME\": \"facebook_profile1\", \"TEXT\": \"Sample text 1\"},\n",
        "            {\"USER ID\": \"facebook_user2\", \"USERNAME\": \"FacebookUser2\", \"CREATED AT\": \"2024-02-14\", \"PROFILE NAME\": \"facebook_profile2\", \"TEXT\": \"Sample text 2\"}\n",
        "        ]\n",
        "\n",
        "# Example usage:\n",
        "api_keys = {\n",
        "    'reddit': 'REDDIT_API_KEY',\n",
        "    'instagram': 'INSTAGRAM_API_KEY',\n",
        "    'twitter': 'TWITTER_API_KEY',\n",
        "    'facebook': 'FACEBOOK_API_KEY'\n",
        "}\n",
        "\n",
        "data_collector = SocialMediaDataCollector(api_keys)\n",
        "\n",
        "# Fetch data from Reddit\n",
        "reddit_data = data_collector.fetch_data('reddit', '#python')\n",
        "\n",
        "# Fetch data from Instagram\n",
        "instagram_data = data_collector.fetch_data('instagram', 'python')\n",
        "\n",
        "# Fetch data from Twitter\n",
        "twitter_data = data_collector.fetch_data('twitter', 'python')\n",
        "\n",
        "# Fetch data from Facebook\n",
        "facebook_data = data_collector.fetch_data('facebook', 'python')\n",
        "\n",
        "# Combine data into a DataFrame with more than four columns\n",
        "all_data = pd.concat([\n",
        "    pd.DataFrame(reddit_data, columns=['USER ID', 'USERNAME', 'CREATED AT', 'PROFILE NAME', 'TEXT']),\n",
        "    pd.DataFrame(instagram_data, columns=['USER ID', 'USERNAME', 'CREATED AT', 'PROFILE NAME', 'TEXT']),\n",
        "    pd.DataFrame(twitter_data, columns=['USER ID', 'USERNAME', 'CREATED AT', 'PROFILE NAME', 'TEXT']),\n",
        "    pd.DataFrame(facebook_data, columns=['USER ID', 'USERNAME', 'CREATED AT', 'PROFILE NAME', 'TEXT'])\n",
        "], ignore_index=True)\n",
        "\n",
        "# Display the combined data if not empty\n",
        "if not all_data.empty:\n",
        "    print(all_data)\n",
        "else:\n",
        "    print(\"No data available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning web scraping for the first time was tough, but with online resources, I managed to grasp the basics but not enough. Understanding HTML structure, CSS selectors, and XPath was tricky initially. I often had to rely on tutorials and forums to troubleshoot problems and experiment with different scraping tools like BeautifulSoup and Scrapy.Overall, while learning web scraping had its challenges. It's opened up new possibilities for research and analysis.\n",
        "\n",
        "# I understood that I have to go through and learn a lot in these topics, I understood at my best as per basic understanding."
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}